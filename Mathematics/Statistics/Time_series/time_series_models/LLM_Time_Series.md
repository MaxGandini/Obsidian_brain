The [[Transformers]] architecture without the tokenizers in the structure can be used as a context-aware prediction model for time series.
It's important to note that the architecture resembles human awareness of context and meaning, so It's no surprise that it is bad for actually predicting stock, but useful for predicting what a competitor in a market is going to do with his strategy. (This is my interpretation, feel free to disagree)

In this new paradigm of time series prediction models, one can choose from [[Hugging Face Transformers]] around 5 standardized models for prediction. 
The following link is about [[Patch Time Series Transformer]]
