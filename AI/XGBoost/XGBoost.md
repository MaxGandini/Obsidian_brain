From the doc:
```
XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.
```

A possible advantage it has, is that the algorithms introduce new [[Hyper-parameter optimization]] options. [[Sci-Kit Learn]], feeds off XGBoost, by having optimized loops in it's calculations.



``