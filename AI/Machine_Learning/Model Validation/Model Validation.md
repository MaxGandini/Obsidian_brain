
![[ttsl.png]]

[[Machine Learning]] models always return an output. We need tools to evaluate the validity of the model to what we are trying to apply.

There's a fundamental distinction between the score calculated with the test_data and the actual score with data you never saw. [[train score]]

- [[Cross Validation]]
- [[Hyper-parameter optimization]] 

Self-made examples:
- [[model_validation_overfit.py]] 
- [[model_validation.py]] 
- 
Where I use [[Scikit-Pipeline]] and [[Cross Validation]]