Everything starts with the paper: 

##### Attention is all you need

First of all, the paper mentions the [[Encoders-Decoders]] architecture.

### Self-Attention concept:

This is a concept aimed at analyzing the meaning of a word through it's context. 
Take the example of a word like : "light"

Light can have many meanings: 
```
- The light was too bright
- The light bulbs are bigger
- This purse is light
- Light soda
```

But how can one compute this? Well, we get the input to the model, then you need to process it so that the information of the context is encoded in some way that it can be re-inputted into the model.

Input: 
- Attention score
- Normalize in attention weight
- update meaning

Re-input.
